# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

HAARRRvest is a data repository for food resource information harvested by Pantry Pirate Radio. It serves as a centralized database of food pantries and assistance locations across the United States, with daily automated updates and an interactive web explorer.

## Key Architecture

### Data Flow
1. Data is published by the HAARRRvest Publisher Service from Pantry Pirate Radio
2. Publisher creates date-based branches and syncs data to this repository
3. Raw JSON data saved to `/daily/YYYY-MM-DD/scrapers/`
4. Latest data mirrored in `/latest/`
5. SQLite database updated in `/sqlite/pantry_pirate_radio.sqlite`
6. Map data exported to `/data/` by the publisher service
7. Interactive explorer served via `index.html` using Datasette-Lite
8. GitHub Pages deployment serves `map.html` for interactive mapping

### Database Schema
The SQLite database follows HSDS (Human Services Data Specification) v3.1.1 standard with 30+ tables. Core tables include:
- `organizations` - Service providers
- `locations` - Physical locations  
- `services` - Types of assistance offered
- `service_at_locations` - Links services to locations
- `addresses`, `phones`, `schedules` - Supporting information
- `location_master` - View combining location data for exports

### Web Interface Components
- `index.html` - Main Datasette-Lite interface for SQL queries
- `map.html` - Interactive map interface (references exported JSON data)
- `explore.html` - Additional exploration interface

## Common Commands

### Export Location Data for Mapping
```bash
python3 scripts/export-locations.py
```
This generates `/data/locations.json` and state-specific files in `/data/states/` from the SQLite database.

### Manual GitHub Pages Deploy
The repository auto-deploys via `.github/workflows/pages.yml` on push to main, but you can trigger manually via GitHub Actions.

## Common Tasks

### Querying Data

SQL via Python:
```python
import sqlite3
conn = sqlite3.connect('sqlite/pantry_pirate_radio.sqlite')
cursor = conn.cursor()
cursor.execute("SELECT * FROM organizations LIMIT 5")
```

DuckDB (for larger queries):
```sql
ATTACH 'sqlite/pantry_pirate_radio.sqlite' AS pantry (TYPE sqlite);
SELECT * FROM pantry.organizations WHERE state = 'CA';
```

### Exploring Raw Data
Raw scraper outputs are in individual JSON files under `/daily/YYYY-MM-DD/scrapers/the_food_pantries_org/` with numeric timestamps. Each file contains one location record with HSDS schema transformation job data.

### Working with the Web Interface
The `index.html` file contains an embedded Datasette-Lite instance. It loads the SQLite database and provides interactive exploration with mapping capabilities. No build process required - it's a static HTML file.

### Data Export Process
The `scripts/export-locations.py` script:
1. Connects to SQLite database
2. Queries `location_master` view for locations with valid coordinates
3. Exports to `/data/locations.json` with metadata
4. Creates state-specific files in `/data/states/` for performance
5. Provides data quality statistics

## Repository Structure

- `/daily/` - Historical data by date with individual JSON files per location
- `/latest/` - Symlink to most recent data
- `/sqlite/` - SQLite database and metadata
- `/data/` - Exported JSON for web mapping (generated by export script)
- `/scripts/` - Python utilities for data processing
- `index.html`, `map.html`, `explore.html` - Web interfaces

## GitHub Actions Workflows

- `pages.yml` - Deploys static site to GitHub Pages on push to main

## Important Notes

1. **This is a pure data repository** - All data processing happens in Pantry Pirate Radio
2. Data updates are pushed by the HAARRRvest Publisher Service from Pantry Pirate Radio
3. The publisher handles:
   - SQLite database generation
   - Map data exports (runs `scripts/export-locations.py`)
   - Git operations (branch creation, merging)
4. All data follows HSDS v3.1.1 specification for interoperability
5. The repository includes historical data dating back to when scraping began
6. Raw data files contain LLM processing jobs that transform scraped data to HSDS format
7. **DO NOT** add any data processing pipelines, scrapers, or automation workflows here
8. Focus improvements on data visualization and accessibility